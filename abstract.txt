UltraRefiner: End-to-End Differentiable Segmentation Refinement for Medical Image Analysis

Abstract

We present UltraRefiner, a novel end-to-end differentiable framework that unifies coarse segmentation and mask refinement into a single jointly optimizable pipeline for medical image segmentation. Unlike existing two-stage approaches that treat segmentation and refinement as independent processes, UltraRefiner enables gradient flow from the refinement network back to the segmentation backbone, allowing both components to co-adapt during training.

================================================================================
1. ARCHITECTURE OVERVIEW
================================================================================

UltraRefiner consists of two main components connected through a differentiable prompt generation module:

1.1 Coarse Segmentation Network: TransUNet
------------------------------------------
We employ TransUNet as our coarse segmentation backbone, a hybrid CNN-Transformer architecture that combines:
- ResNet-50 encoder for hierarchical feature extraction at multiple scales
- Vision Transformer (ViT-B/16) operating on 14x14 patch embeddings for global context modeling
- CNN decoder with skip connections from encoder stages for precise boundary recovery
- Input: Grayscale ultrasound images resized to 224x224
- Output: 2-class probability map (background/foreground) at full resolution

The TransUNet forward pass:
  x_feat = ResNet50_Encoder(x_input)           # Multi-scale features
  x_patch = Patch_Embedding(x_feat)            # 14x14 patches
  x_trans = Transformer_Blocks(x_patch)        # 12 transformer layers
  x_out = CNN_Decoder(x_trans, skip_features)  # Upsample with skips
  p_coarse = Softmax(x_out)[:, 1]              # Foreground probability

1.2 Refinement Network: SAM (Segment Anything Model)
----------------------------------------------------
We adapt SAM/MedSAM for mask refinement with the following components:
- Image Encoder (ViT-B): Processes 1024x1024 RGB images into 64x64x256 embeddings (FROZEN or LoRA-adapted)
- Prompt Encoder: Encodes points, boxes, and masks into sparse/dense embeddings (TRAINABLE)
- Mask Decoder: Lightweight transformer decoder producing 3 candidate masks with IoU scores (TRAINABLE)

SAM input requirements:
- Image: 1024x1024x3 RGB, normalized with ImageNet mean/std
- Point prompts: (x, y) coordinates with labels (1=foreground, 0=background)
- Box prompts: (x1, y1, x2, y2) bounding box coordinates
- Mask prompts: 256x256 low-resolution mask as logits

================================================================================
2. DIFFERENTIABLE PROMPT GENERATION (Key Innovation)
================================================================================

The core technical contribution is our differentiable prompt generator that converts TransUNet's soft probability outputs into SAM-compatible prompts while maintaining gradient flow.

2.1 Soft-Argmax Point Extraction
--------------------------------
Instead of hard argmax (non-differentiable), we compute the expected centroid:

  Given soft mask M of shape (H, W) with values in [0, 1]:

  mask_sum = sum(M) + epsilon                    # Normalization factor
  x_center = sum(M * X_grid) / mask_sum          # Weighted x-coordinate
  y_center = sum(M * Y_grid) / mask_sum          # Weighted y-coordinate

  point_coords = (x_center, y_center)            # Shape: (B, 1, 2)
  point_labels = 1                               # Positive point

For negative points (background), we compute centroid of inverse mask within bounding box:
  inv_mask = (1 - M) * box_mask
  neg_point = soft_argmax(inv_mask)
  neg_label = 0

2.2 Differentiable Bounding Box Extraction
------------------------------------------
We derive box coordinates from mask projections:

  binary_mask = (M > 0.5)                        # Threshold for boundary detection
  y_proj = max(binary_mask, dim=1)              # Project to y-axis
  x_proj = max(binary_mask, dim=0)              # Project to x-axis

  y_indices = nonzero(y_proj)
  x_indices = nonzero(x_proj)

  box = (x_min, y_min, x_max, y_max)            # Shape: (B, 4)

2.3 Continuous Mask Prompt Encoding
-----------------------------------
We preserve the full probability distribution:

  mask_logits = (M * 2 - 1) * 10                 # Scale [0,1] to [-10, 10]
  mask_input = Resize(mask_logits, (256, 256))  # SAM's expected size

  # Shape: (B, 1, 256, 256)

================================================================================
3. THREE-PHASE TRAINING PIPELINE
================================================================================

Phase 1: TransUNet Pre-training (Per-Dataset, 5-Fold CV)
--------------------------------------------------------
Objective: Train robust coarse segmentation models for each dataset.

For each dataset D in {BUSI, BUSBRA, BUS, BUS_UC, BUS_UCLM}:
  For each fold k in {0, 1, 2, 3, 4}:
    train_idx, val_idx = KFold_Split(D.train, k, n_splits=5)

    model = TransUNet(config='R50-ViT-B_16', num_classes=2)
    model.load_pretrained('R50+ViT-B_16.npz')  # ImageNet pretrained

    optimizer = SGD(lr=0.01, momentum=0.9, weight_decay=1e-4)
    scheduler = PolyLR(power=0.9)

    loss = 0.5 * CrossEntropy + 0.5 * DiceLoss

    Train for 150 epochs
    Save best checkpoint based on validation Dice

Output: checkpoints/transunet/{dataset}/fold_{k}/best.pth

Prediction Generation:
----------------------
After training, we inference each fold's validation set:

For each dataset D:
  For each fold k:
    model = load(checkpoints/transunet/{D}/fold_{k}/best.pth)
    for sample in D.train[val_idx_k]:
      pred = model.inference(sample.image)
      save(pred, predictions/{D}/predictions/{sample.name}.npy)

Result: Every training sample has exactly one prediction (from when it was in validation set)

Phase 2: SAM Fine-tuning
========================
We provide TWO approaches for Phase 2, each with different trade-offs:

┌─────────────────────────────────────────────────────────────────────────────┐
│  OPTION A: Using GT with Synthetic Failure Simulation (RECOMMENDED)         │
│  Script: finetune_sam_augmented.py                                          │
│  Pros: Large-scale training data (~100K), diverse failure modes,            │
│        teaches "when to modify" and "when to preserve"                      │
│  Cons: Simulated errors may differ from actual model failures               │
├─────────────────────────────────────────────────────────────────────────────┤
│  OPTION B: Using Actual TransUNet Predictions                               │
│  Script: finetune_sam_with_preds.py                                         │
│  Pros: Realistic coarse masks, matches E2E training distribution            │
│  Cons: Limited training data (~3K), requires Phase 1 completion + inference │
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
Phase 2 Option A: SAM Fine-tuning with Synthetic Failure Simulation [RECOMMENDED]
================================================================================
Objective: Train SAM to refine imperfect masks by simulating realistic segmentation
failures directly on ground-truth masks.

Key Challenge: Teaching the Refiner "When Not to Modify"
--------------------------------------------------------
A critical challenge is ensuring the refiner learns both:
  (1) When TO modify: Low-quality inputs need strong correction
  (2) When NOT to modify: High-quality inputs should be preserved

Failure Mode Simulation
-----------------------
We simulate realistic segmentation failures that match observed model behavior:

Under-segmentation:
  - Boundary erosion: Missing boundary regions due to conservative predictions
  - Partial breakage: Object split into disconnected parts (common for irregular shapes)
  - Small-lesion disappearance: Tiny lesions completely missed (most critical failure)
  - Extreme shrinkage: Severe size reduction from pooling/downsampling artifacts

Over-segmentation:
  - Boundary dilation: Bleeding into background from over-confident predictions
  - Attachment to nearby structures: Unintended connection to adjacent anatomy
  - Artificial bridges: Thin spurious connections to nearby regions

Boundary Artifacts:
  - Contour jitter: Pixel-level noise along edge band from aliasing
  - Edge roughening: Jagged boundaries from low-resolution feature maps
  - Elastic deformation: Wavy/distorted boundaries from spatial inconsistency

Internal Failures:
  - Internal holes: Missing low-contrast central regions
  - Partial dropout: Missing object regions from attention failures

False Positives:
  - False-positive islands: Small spurious blobs near lesion mimicking speckle/artifacts

Size and Shape Conditioning
---------------------------
Augmentation intensity is conditioned on lesion properties to reflect realistic behavior:
  - Tiny lesions (<500 pixels): Higher probability of disappearance/shrinkage
  - Irregular shapes (low circularity): Higher probability of breakage
  - Large lesions: Higher probability of internal holes and partial dropout

SDF-Based Augmentation (Mathematically Grounded)
------------------------------------------------
For anatomically plausible deformations, we operate in the continuous Signed Distance
Function (SDF) domain rather than directly on binary masks:

  1. Convert GT mask M to SDF φ (zero level set defines the contour)
  2. Apply perturbations: φ'(x,y) = φ(x,y) + c + G(x,y)
     - c: global offset for uniform erosion/dilation
     - G(x,y): low-frequency Gaussian random field for spatially varying boundary shifts
  3. Threshold back to binary: M' = (φ' < 0)

This formulation provides:
  - Smooth, anatomically plausible deformations (no pixel-level artifacts)
  - Explicit control over boundary displacement magnitude (in pixels)
  - Topological consistency through level-set representation
  - Regularization via total variation and area constraints

Controlled Dice Score Distribution
----------------------------------
Training samples are generated with controlled quality distribution:
  - Perfect (Dice = 1.0): 10% - Teaches preservation of good predictions
  - High (Dice 0.9-1.0): 15% - Minor artifacts, should mostly preserve
  - Medium (Dice 0.8-0.9): 40% - Moderate errors, needs refinement
  - Low (Dice 0.6-0.8): 35% - Severe failures, needs strong correction

Quality-Aware Training Loss
---------------------------
We introduce a change-penalty term weighted by input mask quality:

  L_total = L_segmentation + λ · L_change_penalty

where L_change_penalty = ||refined - coarse||² × quality_weight(Dice(coarse, GT))

The quality weight increases with input quality:
  - Dice ≈ 0.6 (poor quality): penalty_weight ≈ 0 → allow strong modifications
  - Dice ≈ 1.0 (perfect): penalty_weight ≈ 1 → preserve input, penalize changes

This ensures the refiner learns to make corrections proportional to input quality.

================================================================================
Phase 2 Option B: SAM Fine-tuning (With Actual TransUNet Predictions)
================================================================================
Objective: Adapt SAM to refine TransUNet's actual outputs.

2.1 Data Organization and Pairing
---------------------------------
Data is paired by filename matching across three sources:

  Original Image:    dataset/processed/{dataset}/train/images/{name}.png
  Ground Truth Mask: dataset/processed/{dataset}/train/masks/{name}.png
  TransUNet Pred:    predictions/transunet/{dataset}/predictions/{name}.npy

Example for sample "benign_001":
  image:      dataset/processed/BUSI/train/images/benign_001.png      (grayscale PNG)
  gt_mask:    dataset/processed/BUSI/train/masks/benign_001.png       (binary PNG)
  coarse:     predictions/transunet/BUSI/predictions/benign_001.npy   (float32 [0,1])

The dataset loader automatically pairs these by scanning the image directory
and loading corresponding mask and prediction files with matching names.

2.2 Image Preprocessing for SAM
-------------------------------
SAM requires specific input format different from TransUNet:

Step 1: Load grayscale image
  image_gray = load_png(image_path)              # Shape: (H_orig, W_orig), uint8

Step 2: Resize to SAM input size (1024x1024)
  image_resized = resize(image_gray, (1024, 1024), mode='bilinear')

Step 3: Convert grayscale to RGB (SAM expects 3-channel input)
  image_rgb = stack([image_resized] * 3, axis=-1)  # Shape: (1024, 1024, 3)

Step 4: Normalize to [0, 1] range
  image_norm = image_rgb / 255.0                 # Shape: (1024, 1024, 3), float32

Step 5: Convert to tensor and permute
  image_tensor = torch.tensor(image_norm).permute(2, 0, 1)  # Shape: (3, 1024, 1024)

Step 6: SAM-specific normalization (ImageNet stats, but input in [0,255] range)
  pixel_mean = [123.675, 116.28, 103.53]
  pixel_std = [58.395, 57.12, 57.375]
  image_sam = (image_tensor * 255.0 - pixel_mean) / pixel_std  # Shape: (B, 3, 1024, 1024)

2.3 Coarse Mask Preprocessing
-----------------------------
The TransUNet prediction (coarse mask) needs to be prepared for SAM:

Step 1: Load prediction
  coarse_mask = np.load(pred_path)               # Shape: (224, 224), float32 [0,1]

Step 2: Resize to SAM input size
  coarse_resized = resize(coarse_mask, (1024, 1024), mode='bilinear')
                                                 # Shape: (1024, 1024), float32 [0,1]

Step 3: Convert to tensor
  coarse_tensor = torch.tensor(coarse_resized)   # Shape: (B, 1024, 1024)

2.4 Prompt Generation from Coarse Mask
--------------------------------------
The DifferentiableSAMRefiner extracts three types of prompts:

POINT PROMPTS (Foreground + Background):
  # Positive point: centroid of foreground
  mask_sum = coarse_mask.sum() + 1e-6
  x_pos = (coarse_mask * X_grid).sum() / mask_sum    # Weighted x-coordinate
  y_pos = (coarse_mask * Y_grid).sum() / mask_sum    # Weighted y-coordinate
  point_pos = (x_pos, y_pos)                         # Shape: (B, 1, 2)
  label_pos = 1                                      # Foreground label

  # Negative point: centroid of background within bounding box
  inv_mask = (1 - coarse_mask) * box_region
  x_neg = (inv_mask * X_grid).sum() / inv_mask.sum()
  y_neg = (inv_mask * Y_grid).sum() / inv_mask.sum()
  point_neg = (x_neg, y_neg)                         # Shape: (B, 1, 2)
  label_neg = 0                                      # Background label

  # Combine points
  point_coords = concat([point_pos, point_neg])      # Shape: (B, 2, 2)
  point_labels = concat([label_pos, label_neg])      # Shape: (B, 2)

  # Scale to SAM coordinate system (1024x1024)
  scale = 1024 / 224
  point_coords_scaled = point_coords * scale         # Shape: (B, 2, 2)

BOX PROMPTS:
  # Threshold mask to find boundaries
  binary_mask = (coarse_mask > 0.5)

  # Project to axes and find extent
  y_proj = binary_mask.any(dim=1)                    # Which rows have foreground
  x_proj = binary_mask.any(dim=0)                    # Which cols have foreground

  y_indices = nonzero(y_proj)
  x_indices = nonzero(x_proj)

  y_min, y_max = y_indices.min(), y_indices.max()
  x_min, x_max = x_indices.min(), x_indices.max()

  box = (x_min, y_min, x_max, y_max)                 # Shape: (B, 4)

  # Scale to SAM coordinate system
  box_scaled = box * scale                           # Shape: (B, 4)

MASK PROMPTS:
  # Convert probability to logits for SAM
  mask_logits = (coarse_mask * 2 - 1) * 10           # Scale [0,1] to [-10, 10]

  # Resize to SAM's expected mask input size (256x256)
  mask_input = resize(mask_logits, (256, 256), mode='bilinear')
  mask_input = mask_input.unsqueeze(1)               # Shape: (B, 1, 256, 256)

2.5 SAMRefiner Forward Pass (Detailed)
--------------------------------------
Model configuration:
  sam = load_sam('vit_b', checkpoint='medsam_vit_b.pth')
  sam.image_encoder.freeze()           # 89M params frozen
  sam.prompt_encoder.trainable()       # ~6K params trainable
  sam.mask_decoder.trainable()         # ~4M params trainable

Optional LoRA for image encoder:
  lora_config = {rank: 4, alpha: 4.0, dropout: 0.0}
  inject_lora(sam.image_encoder.blocks, target=['qkv'])
  # Adds ~300K trainable params

Forward pass step-by-step:

  INPUT:
    image_sam: (B, 3, 1024, 1024)    - Normalized image
    coarse_mask: (B, 1024, 1024)     - TransUNet prediction resized

  STEP 1: Image Encoding
    img_embedding = sam.image_encoder(image_sam)
    # Shape: (B, 256, 64, 64)
    # 256 channels, 64x64 spatial (16x downsampled from 1024)

  STEP 2: Prompt Generation
    point_coords, point_labels = extract_points(coarse_mask)
    # point_coords: (B, 2, 2) - [pos_point, neg_point] x [x, y]
    # point_labels: (B, 2) - [1, 0] for [foreground, background]

    box = extract_box(coarse_mask)
    # box: (B, 4) - [x1, y1, x2, y2]

    mask_input = prepare_mask(coarse_mask)
    # mask_input: (B, 1, 256, 256) - logits

  STEP 3: Prompt Encoding
    sparse_embeddings, dense_embeddings = sam.prompt_encoder(
        points=(point_coords, point_labels),
        boxes=box,
        masks=mask_input
    )
    # sparse_embeddings: (B, N_points+2, 256) - point/box embeddings
    # dense_embeddings: (B, 256, 64, 64) - mask embedding

  STEP 4: Mask Decoding
    low_res_masks, iou_predictions = sam.mask_decoder(
        image_embeddings=img_embedding,
        image_pe=sam.prompt_encoder.get_dense_pe(),
        sparse_prompt_embeddings=sparse_embeddings,
        dense_prompt_embeddings=dense_embeddings,
        multimask_output=True
    )
    # low_res_masks: (B, 3, 256, 256) - 3 candidate masks at low resolution
    # iou_predictions: (B, 3) - predicted IoU for each mask

  STEP 5: Mask Selection and Upsampling
    # Select best mask based on predicted IoU
    best_idx = iou_predictions.argmax(dim=1)
    best_mask_low = low_res_masks[batch_idx, best_idx]
    # best_mask_low: (B, 256, 256)

    # Upsample to full resolution
    refined_mask = F.interpolate(
        best_mask_low.unsqueeze(1),
        size=(1024, 1024),
        mode='bilinear'
    ).squeeze(1)
    # refined_mask: (B, 1024, 1024) - logits

    # Apply sigmoid for probability
    refined_prob = torch.sigmoid(refined_mask)
    # refined_prob: (B, 1024, 1024) - probability [0, 1]

  OUTPUT:
    masks_all: (B, 3, 1024, 1024)    - All 3 upsampled candidate masks
    iou_predictions: (B, 3)          - IoU scores for each mask
    refined_mask: (B, 1024, 1024)    - Best mask (highest IoU)

2.6 Loss Function
-----------------
  # Ground truth also resized to 1024x1024
  gt_mask_resized = resize(gt_mask, (1024, 1024))

  # Mask loss (BCE + Dice for each of 3 candidate masks)
  L_bce = BCE_with_logits(masks_all, gt_mask_resized)
  L_dice = 1 - (2 * intersection + 1) / (union + 1)
  L_mask = L_bce + L_dice

  # IoU prediction loss
  iou_actual = compute_iou(sigmoid(masks_all), gt_mask_resized)
  L_iou = MSE(iou_predictions, iou_actual)

  # Total loss
  L_total = 1.0 * L_mask + 1.0 * L_iou

2.7 Validation Metrics Comparison
---------------------------------
During validation, we compute metrics for both coarse and refined masks:

  Coarse metrics: Compare TransUNet prediction vs Ground Truth
  Refined metrics: Compare SAM output vs Ground Truth
  Delta: Refined - Coarse (positive = improvement)

Display format (each epoch):
  ┌────────────────────────────────────────────────────────────┐
  │  [VALIDATION] Coarse (TransUNet) vs Refined (SAM)          │
  │  ============================================================│
  │  Metric       Coarse    Refined      Delta                  │
  │  ------------------------------------------------------------│
  │  Dice         0.7500    0.8200      +0.0700 ↑               │
  │  IoU          0.6000    0.6900      +0.0900 ↑               │
  │  Precision    0.7800    0.8100      +0.0300 ↑               │
  │  Recall       0.7200    0.8300      +0.1100 ↑               │
  │  Accuracy     0.9500    0.9650      +0.0150 ↑               │
  │  ============================================================│
  │  Average improvement: +0.0630 (SAM is helping!)             │
  └────────────────────────────────────────────────────────────┘

TensorBoard logging:
  - val_coarse/dice, val_coarse/iou, ...
  - val_refined/dice, val_refined/iou, ...
  - val_delta/dice, val_delta/iou, ...

Output: checkpoints/sam_finetuned/fold_{k}/best.pth

Phase 3: End-to-End Joint Optimization
--------------------------------------
Objective: Jointly optimize TransUNet and SAM with full gradient flow.

Model initialization:
  transunet = load(checkpoints/transunet/{D}/fold_{k}/best.pth)
  sam_refiner = load(checkpoints/sam_finetuned/fold_{k}/best.pth)
  ultra_refiner = UltraRefiner(transunet, sam_refiner)

Forward pass with gradient flow:
  # TransUNet produces soft mask
  p_coarse = transunet(image_224)                    # (B, H, W)

  # Differentiable prompt generation (gradients flow through)
  prompts = generate_prompts(p_coarse)               # points, boxes, masks

  # SAM refinement
  image_1024 = resize(image_224, 1024)
  img_emb = sam.image_encoder(image_1024)
  sparse_emb, dense_emb = sam.prompt_encoder(prompts)
  p_refined = sam.mask_decoder(img_emb, sparse_emb, dense_emb)

Combined loss:
  L_coarse = CE(p_coarse, gt) + Dice(p_coarse, gt)
  L_refined = BCE(p_refined, gt) + Dice(p_refined, gt)
  L_total = 0.3 * L_coarse + 0.7 * L_refined

Gradient flow:
  dL/d(transunet_params) = dL_coarse/d(params) + dL_refined/d(p_refined) * d(p_refined)/d(prompts) * d(prompts)/d(p_coarse) * d(p_coarse)/d(params)

This enables TransUNet to learn features that produce coarse masks optimized for SAM refinement.

================================================================================
4. DATASETS AND EVALUATION
================================================================================

Datasets:
---------
| Dataset   | Train | Test | Total | Description                    |
|-----------|-------|------|-------|--------------------------------|
| BUSI      | 518   | 129  | 647   | Breast ultrasound (benign+malignant) |
| BUSBRA    | 1500  | 375  | 1875  | Brazilian breast ultrasound    |
| BUS       | 450   | 112  | 562   | Breast ultrasound dataset      |
| BUS_UC    | 765   | 191  | 956   | UC breast ultrasound           |
| BUS_UCLM  | 130   | 33   | 163   | UCLM breast ultrasound         |

Note: Samples with blank masks (no lesion) are automatically excluded during preprocessing.

Evaluation Protocol:
--------------------
- 5-fold cross-validation within training set for model selection
- Held-out test set (20%) for final evaluation
- Metrics: Dice, IoU (Jaccard), Precision, Recall, Accuracy

================================================================================
5. IMPLEMENTATION DETAILS
================================================================================

Data Pipeline:
- Automatic dataset detection and loading
- K-fold split with deterministic seeding for reproducibility
- Data augmentation: random rotation (±20°), horizontal flip
- TransUNet input: 224x224 grayscale, normalized to [0, 1]
- SAM input: 1024x1024 RGB, normalized with ImageNet stats

Training Configuration:
- TransUNet: SGD, lr=0.01, poly scheduler, 150 epochs
- SAM fine-tune: AdamW, lr=1e-4, cosine scheduler, 100 epochs
- End-to-end: AdamW, lr=1e-5, cosine scheduler, 100 epochs

Logging:
- TensorBoard support for loss, metrics, learning rate
- Separate tracking for coarse/refined/delta metrics
- Beautiful console output with epoch summaries

================================================================================
6. CONTRIBUTIONS
================================================================================

1. First end-to-end differentiable framework integrating SAM for medical image segmentation
2. Novel differentiable prompt generation enabling gradient flow through discrete prompts
3. Prediction-based SAM training using actual coarse network outputs
4. LoRA integration for parameter-efficient SAM adaptation to medical domain
5. Comprehensive evaluation on five breast ultrasound datasets with K-fold CV

Our work establishes a new paradigm for integrating foundation models into task-specific pipelines through differentiable interfaces, with broad applicability beyond medical image segmentation to any domain requiring coarse-to-fine prediction refinement.
