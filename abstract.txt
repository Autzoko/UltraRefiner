UltraRefiner: End-to-End Differentiable Segmentation Refinement for Medical Image Analysis

Abstract

We present UltraRefiner, a novel end-to-end differentiable framework that unifies coarse segmentation and mask refinement into a single jointly optimizable pipeline for medical image segmentation. Unlike existing two-stage approaches that treat segmentation and refinement as independent processes, UltraRefiner enables gradient flow from the refinement network back to the segmentation backbone, allowing both components to co-adapt during training.

================================================================================
1. ARCHITECTURE OVERVIEW
================================================================================

UltraRefiner consists of two main components connected through a differentiable prompt generation module:

1.1 Coarse Segmentation Network: TransUNet
------------------------------------------
We employ TransUNet as our coarse segmentation backbone, a hybrid CNN-Transformer architecture that combines:
- ResNet-50 encoder for hierarchical feature extraction at multiple scales
- Vision Transformer (ViT-B/16) operating on 14x14 patch embeddings for global context modeling
- CNN decoder with skip connections from encoder stages for precise boundary recovery
- Input: Grayscale ultrasound images resized to 224x224
- Output: 2-class probability map (background/foreground) at full resolution

The TransUNet forward pass:
  x_feat = ResNet50_Encoder(x_input)           # Multi-scale features
  x_patch = Patch_Embedding(x_feat)            # 14x14 patches
  x_trans = Transformer_Blocks(x_patch)        # 12 transformer layers
  x_out = CNN_Decoder(x_trans, skip_features)  # Upsample with skips
  p_coarse = Softmax(x_out)[:, 1]              # Foreground probability

1.2 Refinement Network: SAM (Segment Anything Model)
----------------------------------------------------
We adapt SAM/MedSAM for mask refinement with the following components:
- Image Encoder (ViT-B): Processes 1024x1024 RGB images into 64x64x256 embeddings (FROZEN or LoRA-adapted)
- Prompt Encoder: Encodes points, boxes, and masks into sparse/dense embeddings (TRAINABLE)
- Mask Decoder: Lightweight transformer decoder producing 3 candidate masks with IoU scores (TRAINABLE)

SAM input requirements:
- Image: 1024x1024x3 RGB, normalized with ImageNet mean/std
- Point prompts: (x, y) coordinates with labels (1=foreground, 0=background)
- Box prompts: (x1, y1, x2, y2) bounding box coordinates
- Mask prompts: 256x256 low-resolution mask as logits

================================================================================
2. DIFFERENTIABLE PROMPT GENERATION (Key Innovation)
================================================================================

The core technical contribution is our differentiable prompt generator that converts
TransUNet's soft probability outputs into SAM-compatible prompts while maintaining
gradient flow. All three prompt types are fully differentiable.

2.1 Point Prompts via Soft-Argmax
---------------------------------
Instead of hard argmax (non-differentiable), we compute expected coordinates using
probability-weighted averages.

**Positive Point (Foreground Centroid):**
Given soft mask P of shape (H, W) with values in [0, 1]:

  mask_sum = Σ P(i,j) + ε                        # Normalization factor
  x_center = Σ (P(i,j) × x_j) / mask_sum         # Weighted x-coordinate
  y_center = Σ (P(i,j) × y_i) / mask_sum         # Weighted y-coordinate

  point_pos = (x_center, y_center)               # Foreground centroid
  label_pos = 1                                  # Positive label

**Negative Point (Background Centroid within Bounding Box):**
To provide a meaningful background point, we compute the centroid of the inverse
mask restricted to the bounding box region:

  # Create soft bounding box mask using sigmoid boundaries
  soft_box = σ((x - x1)/τ) × σ((x2 - x)/τ) × σ((y - y1)/τ) × σ((y2 - y)/τ)

  # Inverse mask within box region
  inv_mask = (1 - P) × soft_box
  inv_sum = Σ inv_mask + ε

  x_neg = Σ (inv_mask × x) / inv_sum
  y_neg = Σ (inv_mask × y) / inv_sum

  point_neg = (x_neg, y_neg)                     # Background centroid
  label_neg = 0                                  # Negative label

**Combined Points:**
  point_coords = [(x_center, y_center), (x_neg, y_neg)]   # Shape: (B, 2, 2)
  point_labels = [1, 0]                                    # Shape: (B, 2)

2.2 Box Prompts via Weighted Statistics
---------------------------------------
Traditional bounding box extraction uses hard thresholding (non-differentiable).
We instead compute box coordinates from mask-weighted coordinate statistics.

**Weighted Centroid:**
  mask_sum = Σ P(i,j) + ε
  center_x = Σ (P × x) / mask_sum                # Weighted mean x
  center_y = Σ (P × y) / mask_sum                # Weighted mean y

**Weighted Standard Deviation:**
  x_diff = x - center_x
  y_diff = y - center_y

  var_x = Σ (P × x_diff²) / mask_sum             # Weighted variance x
  var_y = Σ (P × y_diff²) / mask_sum             # Weighted variance y

  std_x = √(var_x + ε)
  std_y = √(var_y + ε)

**Box Bounds (Center ± k×std, where k=2.5 covers ~99% of probability mass):**
  x1 = center_x - 2.5 × std_x
  x2 = center_x + 2.5 × std_x
  y1 = center_y - 2.5 × std_y
  y2 = center_y + 2.5 × std_y

  # Clamp to image boundaries
  box = (clamp(x1, 0, W), clamp(y1, 0, H), clamp(x2, 0, W), clamp(y2, 0, H))

**Why Weighted Statistics Instead of Soft-Min/Max?**
Soft-min/max approaches (e.g., using softmax over coordinates) can be dominated
by background pixels when the mask is sparse. Our weighted statistics approach
correctly restricts computation to the mask region by using P as weights.

2.3 Mask Prompts via Direct/Gaussian Conversion
-----------------------------------------------
The soft probability mask is converted to SAM's expected logit format.

**Logit Conversion:**
  mask_logits = (P × 2 - 1) × 10                 # Maps [0,1] → [-10, +10]

**Two Styles for Different SAM Checkpoints:**

1. **Gaussian Style** (for unfinetuned SAM/MedSAM):
   - Applies Gaussian blur (σ=3) before logit conversion
   - Creates softer boundaries matching SAM's original training distribution
   - Recommended when skipping Phase 2 (SAM finetuning)

   mask_blurred = GaussianBlur(P, kernel=7, sigma=3)
   mask_logits = (mask_blurred × 2 - 1) × 10

2. **Direct Style** (for Phase 2-finetuned SAM):
   - Passes soft mask directly without blur
   - Preserves TransUNet's output distribution
   - Must match Phase 2 training setting for consistency

   mask_logits = (P × 2 - 1) × 10

**Downsampling to SAM's Expected Size:**
  mask_input = Resize(mask_logits, (256, 256), mode='bilinear')
  # Shape: (B, 1, 256, 256)

================================================================================
2.4 ROI CROPPING MODE (Optional Advanced Feature)
================================================================================

ROI (Region of Interest) cropping focuses SAM's computation on the lesion area
at full 1024×1024 resolution, improving detail preservation for small lesions.

**Motivation:**
When processing the full image at 1024×1024, small lesions may occupy only a
tiny fraction of pixels, limiting SAM's ability to capture fine boundary details.
ROI cropping extracts the lesion region, processes it at full SAM resolution,
then pastes the result back.

**2.4.1 Differentiable ROI Box Extraction**
-------------------------------------------
We use the same weighted statistics approach as box prompts:

  center_x = Σ (P × x) / Σ P
  center_y = Σ (P × y) / Σ P
  std_x = √(Σ (P × (x - center_x)²) / Σ P)
  std_y = √(Σ (P × (y - center_y)²) / Σ P)

  # Expand box by roi_expand_ratio (default 0.2 = 20% on each side)
  expand = roi_expand_ratio
  roi_x1 = center_x - (1 + expand) × 2.5 × std_x
  roi_x2 = center_x + (1 + expand) × 2.5 × std_x
  roi_y1 = center_y - (1 + expand) × 2.5 × std_y
  roi_y2 = center_y + (1 + expand) × 2.5 × std_y

**2.4.2 Differentiable ROI Cropping via grid_sample**
-----------------------------------------------------
Instead of hard indexing (non-differentiable), we use PyTorch's grid_sample
for differentiable spatial transformation:

  # Compute normalized coordinates for grid_sample
  # grid_sample expects coordinates in [-1, 1] range
  grid_x = linspace(roi_x1/W × 2 - 1, roi_x2/W × 2 - 1, 1024)
  grid_y = linspace(roi_y1/H × 2 - 1, roi_y2/H × 2 - 1, 1024)
  grid = meshgrid(grid_x, grid_y)                # Shape: (1024, 1024, 2)

  # Crop image and mask to ROI at full SAM resolution
  image_roi = grid_sample(image, grid, mode='bilinear', align_corners=False)
  mask_roi = grid_sample(coarse_mask, grid, mode='bilinear', align_corners=False)
  # Both shapes: (B, C, 1024, 1024)

**2.4.3 Prompt Generation in ROI Coordinate Space**
---------------------------------------------------
With ROI mode, prompts are generated in the ROI's local coordinate system:

  # Points are extracted from mask_roi (1024×1024 ROI space)
  point_coords_roi = extract_soft_points(mask_roi)   # In [0, 1024] range

  # Box prompt covers the mask within ROI (already full-resolution)
  box_roi = extract_soft_box(mask_roi)               # In [0, 1024] range

  # Mask prompt from ROI mask
  mask_input = prepare_mask(mask_roi)                # Shape: (B, 1, 256, 256)

**2.4.4 SAM Processing in ROI Space**
-------------------------------------
SAM processes the cropped ROI image at full 1024×1024 resolution:

  img_embedding = sam.image_encoder(image_roi)       # Full resolution encoding

  sparse_emb, dense_emb = sam.prompt_encoder(
      points=(point_coords_roi, point_labels),       # ROI coordinates
      boxes=box_roi,                                  # ROI coordinates
      masks=mask_input
  )

  refined_mask_roi = sam.mask_decoder(img_embedding, sparse_emb, dense_emb)
  # Shape: (B, 1024, 1024) in ROI space

**2.4.5 Differentiable ROI Paste-Back**
---------------------------------------
The refined mask is pasted back to the original image space using inverse grid_sample:

  # Create output canvas (zeros = background)
  refined_full = zeros(B, H_original, W_original)

  # Compute inverse grid (from original space to ROI space)
  inv_grid_x = (x - roi_x1) / (roi_x2 - roi_x1) × 2 - 1
  inv_grid_y = (y - roi_y1) / (roi_y2 - roi_y1) × 2 - 1

  # Sample from ROI result using inverse grid
  # Only pixels within ROI bounds get valid values
  refined_full = grid_sample(refined_mask_roi, inv_grid, mode='bilinear')

  # Mask out regions outside ROI (set to background)
  roi_mask = (x >= roi_x1) & (x <= roi_x2) & (y >= roi_y1) & (y <= roi_y2)
  refined_full = refined_full × roi_mask

**2.4.6 Complete ROI Pipeline**
-------------------------------
  INPUT: image (B, 3, 1024, 1024), coarse_mask (B, 1024, 1024)

  Step 1: Extract ROI box from coarse_mask
          roi_box = extract_roi_box(coarse_mask, expand_ratio=0.2)

  Step 2: Crop image and mask to ROI
          image_roi = differentiable_crop(image, roi_box, size=1024)
          mask_roi = differentiable_crop(coarse_mask, roi_box, size=1024)

  Step 3: Generate prompts in ROI space
          points_roi = extract_soft_points(mask_roi)     # ROI coordinates
          box_roi = extract_soft_box(mask_roi)           # ROI coordinates
          mask_prompt = prepare_mask(mask_roi)           # 256×256 logits

  Step 4: SAM forward pass on ROI
          img_emb = sam.image_encoder(image_roi)
          refined_roi = sam.mask_decoder(img_emb, points_roi, box_roi, mask_prompt)

  Step 5: Paste refined mask back to original space
          refined_full = differentiable_paste(refined_roi, roi_box, original_size)

  OUTPUT: refined_full (B, 1024, 1024)

**Phase 2 ↔ Phase 3 ROI Consistency:**
If ROI mode is used in Phase 3, it MUST also be used in Phase 2 with the same
roi_expand_ratio. This ensures SAM sees the same input distribution during
finetuning and end-to-end training.

================================================================================
2.5 GATED RESIDUAL REFINEMENT (Alternative to Phase 2)
================================================================================

**2.5.0 Design Philosophy and Motivation**
------------------------------------------
The standard UltraRefiner pipeline assumes SAM has been finetuned (Phase 2) to
understand the medical imaging domain and coarse mask distribution. However,
Phase 2 requires significant computational resources and careful hyperparameter
tuning. When Phase 2 is skipped, a fundamental problem emerges:

  THE UNFINETUNED SAM PROBLEM:
  ┌────────────────────────────────────────────────────────────────────────────┐
  │ SAM was trained on natural images with high-quality prompts. When given    │
  │ soft probability masks from TransUNet as prompts:                          │
  │                                                                            │
  │ 1. Distribution Mismatch: SAM expects sharp, confident masks but receives  │
  │    soft probabilities with uncertainty at boundaries                       │
  │                                                                            │
  │ 2. Domain Gap: Medical ultrasound characteristics (speckle noise, low      │
  │    contrast, fuzzy boundaries) differ from SAM's training distribution     │
  │                                                                            │
  │ 3. Overconfident Corrections: SAM may "correct" already accurate regions,  │
  │    introducing false positives/negatives in confident predictions          │
  │                                                                            │
  │ 4. No "Do Nothing" Training: SAM never learned when to preserve input —    │
  │    it always tries to produce a "better" mask, even when input is perfect  │
  └────────────────────────────────────────────────────────────────────────────┘

  OBSERVED FAILURE MODE (without gating):
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │ TransUNet Dice: 0.85 → SAM Refinement Dice: 0.72 (DEGRADATION!)             │
  │                                                                             │
  │ SAM incorrectly "refines" confident TransUNet predictions, replacing       │
  │ accurate segmentations with hallucinated boundaries based on its natural   │
  │ image priors. The refinement network actively hurts performance.           │
  └─────────────────────────────────────────────────────────────────────────────┘

**THE GATED RESIDUAL SOLUTION:**

Instead of letting SAM directly replace the coarse prediction, we constrain it
to act as a RESIDUAL ERROR CORRECTOR with spatially-varying correction strength:

  Standard:  final = SAM(coarse)                    — Direct replacement
  Gated:     final = coarse + gate × (SAM - coarse) — Controlled correction
                            └───────────────────┘
                              Residual (correction)

The gate (ranging 0 to 1) controls how much we trust SAM's correction at each
pixel. This design embodies several key principles:

  PRINCIPLE 1: PRESERVE BY DEFAULT
  ────────────────────────────────
  When gate ≈ 0, final ≈ coarse. Confident predictions (coarse near 0 or 1)
  are preserved, preventing SAM from damaging accurate segmentations.

  PRINCIPLE 2: CORRECT SELECTIVELY
  ────────────────────────────────
  When gate ≈ 1, final ≈ SAM. Uncertain predictions (coarse ≈ 0.5) are
  delegated to SAM for refinement, leveraging SAM's boundary expertise.

  PRINCIPLE 3: SMOOTH INTERPOLATION
  ─────────────────────────────────
  The gate provides smooth interpolation between coarse and SAM outputs,
  avoiding harsh transitions and enabling gradient-based optimization.

  PRINCIPLE 4: UNCERTAINTY-AWARE
  ──────────────────────────────
  The gate is derived from TransUNet's own prediction confidence. Regions
  where TransUNet is uncertain (boundary pixels) get refined; regions
  where TransUNet is confident (interior/exterior) are preserved.

**Mathematical Formulation:**

  final = coarse + gate × (SAM - coarse)
        = coarse + gate × residual
        = (1 - gate) × coarse + gate × SAM    [equivalent form]

  where:
    coarse ∈ [0, 1]^{H×W}  — TransUNet soft mask (probabilities)
    SAM ∈ [0, 1]^{H×W}     — SAM refinement output (probabilities)
    gate ∈ [0, 1]^{H×W}    — Spatially-varying correction strength
    residual = SAM - coarse — SAM's proposed correction

**Why Residual Learning?**

The residual formulation (coarse + gate × residual) is inspired by ResNet:
- Easier to learn small corrections than complete transformations
- Identity mapping (gate=0) is the default, requiring no learning
- Gradients flow directly through the skip connection
- The network learns "what to change" rather than "what the answer is"

**Core Formula:**
Instead of: final = SAM(coarse)                    [direct replacement]
We use:     final = coarse + gate × (SAM - coarse) [controlled correction]

This can be rewritten as:
  final = (1 - gate) × coarse + gate × SAM

When gate ≈ 0: final ≈ coarse (preserve accurate predictions)
When gate ≈ 1: final ≈ SAM    (trust SAM's correction)

**Pipeline Comparison:**

STANDARD ULTRAREFINER (Direct Replacement):
┌──────────────────────────────────────────────────────────────────────────────┐
│                                                                              │
│   Image ──→ TransUNet ──→ Coarse ──→ Prompts ──→ SAM ──→ Final              │
│             (frozen)       Mask                          Mask               │
│                             │                             │                 │
│                             └───────── IGNORED ───────────┘                 │
│                                                                              │
│   Problem: SAM completely replaces coarse mask, may degrade good predictions│
└──────────────────────────────────────────────────────────────────────────────┘

GATED ULTRAREFINER (Controlled Correction):
┌──────────────────────────────────────────────────────────────────────────────┐
│                                                                              │
│   Image ──→ TransUNet ──→ Coarse ──┬──→ Prompts ──→ SAM ──→ SAM Mask        │
│             (frozen)       Mask    │                          │              │
│                             │      │                          │              │
│                             │      └──→ Uncertainty ──→ Gate  │              │
│                             │               │           │     │              │
│                             │               ▼           ▼     ▼              │
│                             └──────────→ [ Coarse + Gate × (SAM - Coarse) ] │
│                                                      │                       │
│                                                      ▼                       │
│                                                 Final Mask                   │
│                                                                              │
│   Benefit: Preserves confident predictions, only refines uncertain regions  │
└──────────────────────────────────────────────────────────────────────────────┘

**Information Flow in Gated Refinement:**

  ┌─────────────┐
  │   Coarse    │──────────────────────────────────────────┐
  │    Mask     │                                          │
  │  (0.0-1.0)  │──────┐                                   │
  └─────────────┘      │                                   │
        │              │                                   │
        │              ▼                                   │
        │    ┌─────────────────┐                           │
        │    │ Compute Gate:   │                           │
        │    │ confidence =    │                           │
        │    │  |2×coarse - 1| │                           │
        │    │ gate = 1 - conf │                           │
        │    └────────┬────────┘                           │
        │             │                                    │
        │             │ gate ∈ [0, 1]                      │
        │             │                                    │
        ▼             │                                    │
  ┌─────────────┐     │                                    │
  │   Prompt    │     │                                    │
  │ Generation  │     │                                    │
  └──────┬──────┘     │                                    │
         │            │                                    │
         ▼            │                                    │
  ┌─────────────┐     │                                    │
  │     SAM     │     │                                    │
  │  Refinement │     │                                    │
  └──────┬──────┘     │                                    │
         │            │                                    │
         │ SAM output │                                    │
         │ (0.0-1.0)  │                                    │
         │            │                                    │
         ▼            ▼                                    ▼
  ┌───────────────────────────────────────────────────────────────┐
  │                                                               │
  │   Final = Coarse + Gate × (SAM - Coarse)                      │
  │                          └─────┬─────┘                        │
  │                             Residual                          │
  │                                                               │
  │   At each pixel:                                              │
  │   • If coarse ≈ 0.5 (uncertain): gate ≈ 1 → Final ≈ SAM       │
  │   • If coarse ≈ 0 or 1 (confident): gate ≈ 0 → Final ≈ Coarse │
  │                                                               │
  └───────────────────────────────────────────────────────────────┘

**2.5.1 Uncertainty-Based Gate**
--------------------------------
The gate is computed from the coarse mask's pixel-wise uncertainty:

  confidence = |2 × coarse - 1|           # 0 at coarse=0.5, 1 at coarse=0 or 1
  uncertainty = 1 - confidence^γ          # High when coarse is uncertain
  gate = gate_min + (gate_max - gate_min) × uncertainty

Gate behavior by coarse value:
┌──────────────────────────────────────────────────────────────────┐
│  Coarse Value    Confidence    Uncertainty    Gate (γ=1, max=0.8)│
├──────────────────────────────────────────────────────────────────┤
│  0.0 (certain bg)    1.0          0.0             0.0            │
│  0.2 (likely bg)     0.6          0.4             0.32           │
│  0.5 (uncertain)     0.0          1.0             0.8            │
│  0.8 (likely fg)     0.6          0.4             0.32           │
│  1.0 (certain fg)    1.0          0.0             0.0            │
└──────────────────────────────────────────────────────────────────┘

**Parameters:**
- γ (gamma): Controls uncertainty curve shape
  - γ > 1: More aggressive gating (only very uncertain regions get corrections)
  - γ < 1: Softer gating (more regions receive SAM corrections)
  - γ = 1: Linear relationship (default)

- gate_min: Minimum gate value (default 0.0)
  - 0.0 = Fully preserve confident regions
  - >0 = Allow small corrections everywhere

- gate_max: Maximum gate value (default 0.8)
  - 1.0 = Full SAM replacement in uncertain regions
  - <1.0 = Cap maximum correction strength (recommended for unfinetuned SAM)

**2.5.2 Learned Gate (Optional)**
---------------------------------
For more complex correction patterns, a learned gate network predicts where
corrections should be applied:

  Input: [coarse, sam_output, uncertainty]    # Shape: (B, 3, H, W)

  Gate Network:
    Conv2d(3, 32, 3×3) → ReLU →
    Conv2d(32, 32, 3×3) → ReLU →
    Conv2d(32, 1, 1×1) → Sigmoid

  Output: gate                                 # Shape: (B, H, W)

The network is initialized conservatively (small initial gate values) to
preserve coarse predictions by default.

**2.5.3 Hybrid Gate**
---------------------
Combines uncertainty and learned gates multiplicatively:

  gate_hybrid = gate_uncertainty × gate_learned

This ensures corrections only occur where both:
1. The coarse prediction is uncertain (uncertainty gate high)
2. The learned network predicts a correction is beneficial (learned gate high)

**2.5.4 Differentiability**
---------------------------
All gating operations are fully differentiable:

  ∂final/∂coarse = (1 - gate) + gate × ∂SAM/∂coarse + (SAM - coarse) × ∂gate/∂coarse
  ∂final/∂SAM = gate

Gradients flow through:
1. The direct coarse path (weighted by 1-gate)
2. The SAM refinement path (weighted by gate)
3. The gate computation (modulates the residual)

**2.5.5 When to Use Gated Refinement**
--------------------------------------
┌────────────────────────────────────────────────────────────────────────┐
│ Scenario                              │ Recommendation                 │
├────────────────────────────────────────────────────────────────────────┤
│ Skip Phase 2 completely               │ Gated with gate_max=0.5       │
│ Unfinetuned SAM makes wild guesses    │ Gated with gate_max=0.3-0.5   │
│ Phase 2-finetuned SAM                 │ Standard (no gating needed)   │
│ TransUNet already very accurate       │ Gated with gate_max=0.3       │
│ Want to preserve good + fix bad       │ Gated with learned gate       │
└────────────────────────────────────────────────────────────────────────┘

**2.5.6 Implementation**
------------------------
  from models.ultra_refiner import build_gated_ultra_refiner

  model = build_gated_ultra_refiner(
      transunet_checkpoint='./checkpoints/transunet/best.pth',
      sam_checkpoint='./pretrained/medsam_vit_b.pth',
      gate_type='uncertainty',    # 'uncertainty', 'learned', or 'hybrid'
      gate_gamma=1.0,             # Uncertainty curve shape
      gate_min=0.0,               # Minimum correction
      gate_max=0.8,               # Maximum correction (cap at 80%)
      mask_prompt_style='gaussian',
      use_roi_crop=True,
  )

  output = model(image)
  refined_mask = output['refined_mask']  # Final gated output
  gate = output['gate']                  # Visualization of where corrections applied

================================================================================
3. THREE-PHASE TRAINING PIPELINE
================================================================================

Phase 1: TransUNet Pre-training (Per-Dataset, 5-Fold CV)
--------------------------------------------------------
Objective: Train robust coarse segmentation models for each dataset.

For each dataset D in {BUSI, BUSBRA, BUS, BUS_UC, BUS_UCLM}:
  For each fold k in {0, 1, 2, 3, 4}:
    train_idx, val_idx = KFold_Split(D.train, k, n_splits=5)

    model = TransUNet(config='R50-ViT-B_16', num_classes=2)
    model.load_pretrained('R50+ViT-B_16.npz')  # ImageNet pretrained

    optimizer = SGD(lr=0.01, momentum=0.9, weight_decay=1e-4)
    scheduler = PolyLR(power=0.9)

    loss = 0.5 * CrossEntropy + 0.5 * DiceLoss

    Train for 150 epochs
    Save best checkpoint based on validation Dice

Output: checkpoints/transunet/{dataset}/fold_{k}/best.pth

Prediction Generation:
----------------------
After training, we inference each fold's validation set:

For each dataset D:
  For each fold k:
    model = load(checkpoints/transunet/{D}/fold_{k}/best.pth)
    for sample in D.train[val_idx_k]:
      pred = model.inference(sample.image)
      save(pred, predictions/{D}/predictions/{sample.name}.npy)

Result: Every training sample has exactly one prediction (from when it was in validation set)

Phase 2: SAM Fine-tuning (Hybrid Approach)
==========================================
We adopt a HYBRID training strategy that combines the best of both worlds:
50% real TransUNet predictions and 50% synthetically augmented GT masks.

┌─────────────────────────────────────────────────────────────────────────────┐
│  HYBRID APPROACH (USED IN PRACTICE)                                         │
│  Script: finetune_sam_hybrid.py                                             │
│                                                                             │
│  Data Mix: 50% real TransUNet predictions + 50% augmented GT masks          │
│  Pros: Combines realistic coarse masks with diverse failure modes,          │
│        balances distribution coverage with training data volume             │
│  Training: All 5 datasets combined (BUSI, BUSBRA, BUS, BUS_UC, BUS_UCLM)  │
│  SAM Backbone: SAM ViT-B (sam_vit_b_01ec64.pth)                            │
│  Mask Prompt: Direct style (no Gaussian blur)                               │
│  ROI Cropping: Enabled (expand_ratio=0.5)                                  │
│  Epochs: 300, Batch Size: 4, Mixed Precision (AMP)                         │
│  Augmentor Preset: default (12 error types, balanced distribution)          │
└─────────────────────────────────────────────────────────────────────────────┘

The hybrid approach addresses the limitations of pure prediction-based or pure
augmentation-based training:
- Real predictions (50%): Ensure SAM learns the actual TransUNet error distribution
- Augmented GT (50%): Provide diverse failure modes and teach "when to preserve"

Two alternative approaches are also supported:

================================================================================
Phase 2 Alternative A: SAM Fine-tuning with Pure Online Mask Augmentation
================================================================================
Objective: Train SAM to refine imperfect masks by simulating realistic segmentation
failures directly on ground-truth masks using on-the-fly augmentation.

Key Challenge: Teaching the Refiner "When Not to Modify"
--------------------------------------------------------
A critical challenge is ensuring the refiner learns both:
  (1) When TO modify: Low-quality inputs need strong correction
  (2) When NOT to modify: High-quality inputs should be preserved

================================================================================
3.1 COMPREHENSIVE MASK AUGMENTATION SYSTEM
================================================================================

We implement a comprehensive mask augmentation system with 12 primary error types
that simulate realistic TransUNet failure modes. The system applies augmentation
on-the-fly during training, providing unlimited diversity.

**Module Location:** data/mask_augmentation.py, data/online_augmented_dataset.py

3.1.1 Architecture Overview
---------------------------
  ┌─────────────────────────────────────────────────────────────────────────┐
  │                    Mask Augmentation Pipeline                            │
  ├─────────────────────────────────────────────────────────────────────────┤
  │                                                                          │
  │   GT Mask (binary)                                                       │
  │        │                                                                 │
  │        ▼                                                                 │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │              Sample Primary Error Type (1 of 12)                 │   │
  │   │   Probabilities weighted by DEFAULT_ERROR_PROBS                 │   │
  │   └─────────────────────────────┬───────────────────────────────────┘   │
  │                                 │                                        │
  │                                 ▼                                        │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │              Apply 0-2 Secondary Perturbations (50% prob)        │   │
  │   │   - boundary_jitter: morphological noise on boundaries          │   │
  │   │   - threshold_fluctuation: slight dilation/erosion              │   │
  │   └─────────────────────────────┬───────────────────────────────────┘   │
  │                                 │                                        │
  │                                 ▼                                        │
  │   ┌─────────────────────────────────────────────────────────────────┐   │
  │   │              Convert to Soft Mask (80% prob)                     │   │
  │   │   P(x) = sigmoid(signed_distance(x) / temperature)              │   │
  │   │   temperature ~ Uniform(2.0, 8.0)                               │   │
  │   └─────────────────────────────┬───────────────────────────────────┘   │
  │                                 │                                        │
  │                                 ▼                                        │
  │   Pseudo-Coarse Mask M̃ (soft probability, simulating TransUNet)         │
  │                                                                          │
  └─────────────────────────────────────────────────────────────────────────┘

3.1.2 Twelve Primary Error Types
--------------------------------
Each error type simulates a specific TransUNet failure mode:

┌────────────────────────────────────────────────────────────────────────────┐
│  #  │ Error Type              │ Prob  │ Description                        │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  1  │ Identity/Near-Perfect   │ 15%   │ GT or minimal 1-2px boundary shift │
│     │                         │       │ Teaches SAM to preserve good masks │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  2  │ Over-Segmentation       │ 17%   │ Dilation: 1.2x-3x area expansion   │
│     │                         │       │ Simulates bleeding into background │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  3  │ Giant Over-Segmentation │ 10%   │ Massive dilation: 3x-20x expansion │
│     │                         │       │ Background takeover failures       │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  4  │ Under-Segmentation      │ 17%   │ Erosion: 0.4x-0.9x area shrinkage  │
│     │                         │       │ Missing boundary regions           │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  5  │ Missing Chunk           │ 12%   │ Wedge or blob cutout: 5-30% of GT  │
│     │                         │       │ Partial boundary/region missing    │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  6  │ Internal Holes          │ 10%   │ 1-3 elliptical holes inside mask   │
│     │                         │       │ Each hole: 2-20% of GT area        │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  7  │ Bridge/Adhesion         │ 10%   │ Thin band (2-6px × 10-60px) +      │
│     │                         │       │ optional FP blob at end            │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  8  │ False Positive Islands  │ 17%   │ 1-5 blobs (normal) or 5-30 (severe)│
│     │                         │       │ Each blob: 0.2-10% of GT area      │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│  9  │ Fragmentation           │ 10%   │ 1-5 black strips cutting through GT│
│     │                         │       │ Strip width: 2-8px                 │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│ 10  │ Shift/Wrong Location    │ 10%   │ Translation: 5-30% of bbox size    │
│     │                         │       │ Optional rotation: ±5-20°          │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│ 11  │ Empty Prediction        │  4%   │ Completely empty or 1-5% fragment  │
│     │                         │       │ Simulates complete miss            │
├─────┼─────────────────────────┼───────┼────────────────────────────────────┤
│ 12  │ Noise-Only Scatter      │  3%   │ 10-50 tiny blobs, no GT overlap    │
│     │                         │       │ Pure false positive noise          │
└─────┴─────────────────────────┴───────┴────────────────────────────────────┘

3.1.3 Secondary Perturbations
-----------------------------
After primary error, 0-2 secondary perturbations are applied (50% probability):

  boundary_jitter:
    - Applies morphological operations (dilate/erode/open/close)
    - Random kernel size: 3-11 pixels
    - Simulates small boundary instabilities

  threshold_fluctuation:
    - Very slight dilation or erosion (3×3 kernel)
    - Simulates prediction threshold instability

3.1.4 Soft Mask Conversion
--------------------------
80% of augmented masks are converted to soft pseudo-probability maps using
signed distance transform:

  Step 1: Compute signed distance
    d_inside = distance_transform_edt(binary_mask)
    d_outside = distance_transform_edt(1 - binary_mask)
    signed_dist = d_inside - d_outside    # Positive inside, negative outside

  Step 2: Apply sigmoid with temperature
    P(x) = 1 / (1 + exp(-signed_dist(x) / temperature))

  Temperature is sampled from Uniform(2.0, 8.0):
    - Low temperature (2.0): Sharp boundaries, high confidence
    - High temperature (8.0): Soft/fuzzy boundaries, gradual transitions

  This creates realistic soft probability maps that match TransUNet's output
  distribution, with smooth transitions at boundaries.

3.1.5 Preset Configurations
---------------------------
Five presets are available for different training scenarios:

  'default':        Balanced distribution (as shown in table above)
  'mild':           More identity (30%), less severe errors
  'severe':         More giant over-seg (20%), empty (3%), scatter (2%)
  'boundary_focus': 50% over/under-segmentation for boundary refinement
  'structural':     40% holes + missing chunks + fragmentation

3.1.6 Usage Example
-------------------
  from data import create_augmentor, MaskAugmentor, AUGMENTOR_PRESETS

  # Create augmentor with default settings
  augmentor = create_augmentor(preset='default', soft_mask_prob=0.8)

  # Apply augmentation to GT mask
  coarse_mask, aug_info = augmentor(gt_mask)

  # aug_info contains:
  #   'error_type': 'over_segmentation'
  #   'secondary': ['boundary_jitter']
  #   'soft': True
  #   'dice': 0.72

================================================================================
3.2 ONLINE AUGMENTED DATASET
================================================================================

The OnlineAugmentedDataset applies augmentation on-the-fly during training,
providing unlimited augmentation diversity without pre-generating data.

**Benefits:**
  - New augmentation every epoch (unlimited diversity)
  - No disk storage required for augmented masks
  - Easy to adjust augmentation distribution during training
  - Matches Phase 3 prompt generation logic exactly

3.2.1 Dataset Structure
-----------------------
  data_root/
  └── {dataset}/
      └── train/
          ├── images/    # Original images (.png, .jpg, .npy)
          └── masks/     # Ground truth masks (.png)

3.2.2 Resolution Path Matching
------------------------------
The dataset simulates Phase 3's resolution path to ensure training-inference
consistency:

  Step 1: Load image and GT mask at original resolution
  Step 2: Apply mask augmentation (produces soft coarse mask)
  Step 3: Resize to TransUNet resolution (e.g., 224×224)
  Step 4: Resize to SAM resolution (1024×1024)

This bilinear interpolation path matches what happens in Phase 3 when
TransUNet's output is upscaled for SAM, ensuring SAM sees the same input
distribution during training and inference.

3.2.3 Usage Example
-------------------
  from data import get_online_augmented_dataloaders, OnlineAugmentedDataset

  # Get train and val dataloaders
  train_loader, val_loader = get_online_augmented_dataloaders(
      data_root='./dataset/processed',
      dataset_names=['BUSI', 'BUSBRA'],   # Can combine multiple datasets
      batch_size=4,
      img_size=1024,                       # SAM input size
      transunet_img_size=224,              # TransUNet resolution for path matching
      augmentor_preset='default',
      num_workers=4,
  )

  # Each batch contains:
  #   'image':        (B, 3, 1024, 1024)  - SAM-normalized image
  #   'label':        (B, 1024, 1024)     - GT mask
  #   'coarse_mask':  (B, 1024, 1024)     - Augmented soft mask
  #   'dice':         float               - Dice(coarse, GT)
  #   'error_type':   str                 - Primary error type applied
  #   'augmentations': list               - Secondary perturbations

3.2.4 Multi-Dataset Training
----------------------------
The MultiDatasetOnlineAugmented class combines multiple datasets:

  train_dataset = MultiDatasetOnlineAugmented(
      data_root='./dataset/processed',
      dataset_names=['BUSI', 'BUSBRA', 'BUS', 'BUS_UC', 'BUS_UCLM'],
      augmentor_preset='default',
  )

  # Combines all datasets with shared augmentor
  # Outputs: "Combined dataset: 3363 samples from 5 datasets"

================================================================================
3.3 QUALITY-AWARE TRAINING CONSIDERATIONS
================================================================================

Controlled Dice Score Distribution
----------------------------------
The augmentation system naturally produces a distribution of Dice scores:
  - Identity (15%): Dice ≈ 0.95-1.0 (teaches preservation)
  - Over/Under-seg (34%): Dice ≈ 0.60-0.90 (moderate correction)
  - Structural errors (32%): Dice ≈ 0.50-0.85 (varies by severity)
  - Severe cases (7%): Dice ≈ 0.0-0.30 (extreme failures)

Quality-Aware Training Loss
---------------------------
We introduce a change-penalty term weighted by input mask quality:

  L_total = L_segmentation + λ · L_change_penalty

where L_change_penalty = ||refined - coarse||² × quality_weight(Dice(coarse, GT))

The quality weight increases with input quality:
  - Dice ≈ 0.6 (poor quality): penalty_weight ≈ 0 → allow strong modifications
  - Dice ≈ 1.0 (perfect): penalty_weight ≈ 1 → preserve input, penalize changes

This ensures the refiner learns to make corrections proportional to input quality.

================================================================================
Phase 2 Alternative B: SAM Fine-tuning with Pure TransUNet Predictions
================================================================================
Objective: Adapt SAM to refine TransUNet's actual outputs.

2.1 Data Organization and Pairing
---------------------------------
Data is paired by filename matching across three sources:

  Original Image:    dataset/processed/{dataset}/train/images/{name}.png
  Ground Truth Mask: dataset/processed/{dataset}/train/masks/{name}.png
  TransUNet Pred:    predictions/transunet/{dataset}/predictions/{name}.npy

Example for sample "benign_001":
  image:      dataset/processed/BUSI/train/images/benign_001.png      (grayscale PNG)
  gt_mask:    dataset/processed/BUSI/train/masks/benign_001.png       (binary PNG)
  coarse:     predictions/transunet/BUSI/predictions/benign_001.npy   (float32 [0,1])

The dataset loader automatically pairs these by scanning the image directory
and loading corresponding mask and prediction files with matching names.

2.2 Image Preprocessing for SAM
-------------------------------
SAM requires specific input format different from TransUNet:

Step 1: Load grayscale image
  image_gray = load_png(image_path)              # Shape: (H_orig, W_orig), uint8

Step 2: Resize to SAM input size (1024x1024)
  image_resized = resize(image_gray, (1024, 1024), mode='bilinear')

Step 3: Convert grayscale to RGB (SAM expects 3-channel input)
  image_rgb = stack([image_resized] * 3, axis=-1)  # Shape: (1024, 1024, 3)

Step 4: Normalize to [0, 1] range
  image_norm = image_rgb / 255.0                 # Shape: (1024, 1024, 3), float32

Step 5: Convert to tensor and permute
  image_tensor = torch.tensor(image_norm).permute(2, 0, 1)  # Shape: (3, 1024, 1024)

Step 6: SAM-specific normalization (ImageNet stats, but input in [0,255] range)
  pixel_mean = [123.675, 116.28, 103.53]
  pixel_std = [58.395, 57.12, 57.375]
  image_sam = (image_tensor * 255.0 - pixel_mean) / pixel_std  # Shape: (B, 3, 1024, 1024)

2.3 Coarse Mask Preprocessing
-----------------------------
The TransUNet prediction (coarse mask) needs to be prepared for SAM:

Step 1: Load prediction
  coarse_mask = np.load(pred_path)               # Shape: (224, 224), float32 [0,1]

Step 2: Resize to SAM input size
  coarse_resized = resize(coarse_mask, (1024, 1024), mode='bilinear')
                                                 # Shape: (1024, 1024), float32 [0,1]

Step 3: Convert to tensor
  coarse_tensor = torch.tensor(coarse_resized)   # Shape: (B, 1024, 1024)

2.4 Prompt Generation from Coarse Mask
--------------------------------------
The DifferentiableSAMRefiner extracts three types of prompts:

POINT PROMPTS (Foreground + Background):
  # Positive point: centroid of foreground
  mask_sum = coarse_mask.sum() + 1e-6
  x_pos = (coarse_mask * X_grid).sum() / mask_sum    # Weighted x-coordinate
  y_pos = (coarse_mask * Y_grid).sum() / mask_sum    # Weighted y-coordinate
  point_pos = (x_pos, y_pos)                         # Shape: (B, 1, 2)
  label_pos = 1                                      # Foreground label

  # Negative point: centroid of background within bounding box
  inv_mask = (1 - coarse_mask) * box_region
  x_neg = (inv_mask * X_grid).sum() / inv_mask.sum()
  y_neg = (inv_mask * Y_grid).sum() / inv_mask.sum()
  point_neg = (x_neg, y_neg)                         # Shape: (B, 1, 2)
  label_neg = 0                                      # Background label

  # Combine points
  point_coords = concat([point_pos, point_neg])      # Shape: (B, 2, 2)
  point_labels = concat([label_pos, label_neg])      # Shape: (B, 2)

  # Scale to SAM coordinate system (1024x1024)
  scale = 1024 / 224
  point_coords_scaled = point_coords * scale         # Shape: (B, 2, 2)

BOX PROMPTS (Weighted Statistics - Fully Differentiable):
  # Compute weighted centroid
  mask_sum = coarse_mask.sum() + 1e-6
  center_x = (coarse_mask * X_grid).sum() / mask_sum
  center_y = (coarse_mask * Y_grid).sum() / mask_sum

  # Compute weighted standard deviation
  x_diff = X_grid - center_x
  y_diff = Y_grid - center_y
  var_x = (coarse_mask * x_diff**2).sum() / mask_sum
  var_y = (coarse_mask * y_diff**2).sum() / mask_sum
  std_x = sqrt(var_x + 1e-6)
  std_y = sqrt(var_y + 1e-6)

  # Box from center ± 2.5*std (covers ~99% of probability mass)
  x_min = center_x - 2.5 * std_x
  x_max = center_x + 2.5 * std_x
  y_min = center_y - 2.5 * std_y
  y_max = center_y + 2.5 * std_y

  # Clamp to image boundaries
  box = (clamp(x_min, 0, W), clamp(y_min, 0, H),
         clamp(x_max, 0, W), clamp(y_max, 0, H))    # Shape: (B, 4)

  # Scale to SAM coordinate system
  box_scaled = box * scale                           # Shape: (B, 4)

MASK PROMPTS:
  # Convert probability to logits for SAM
  mask_logits = (coarse_mask * 2 - 1) * 10           # Scale [0,1] to [-10, 10]

  # Resize to SAM's expected mask input size (256x256)
  mask_input = resize(mask_logits, (256, 256), mode='bilinear')
  mask_input = mask_input.unsqueeze(1)               # Shape: (B, 1, 256, 256)

2.5 SAMRefiner Forward Pass (Detailed)
--------------------------------------
Model configuration:
  sam = load_sam('vit_b', checkpoint='medsam_vit_b.pth')
  sam.image_encoder.freeze()           # 89M params frozen
  sam.prompt_encoder.trainable()       # ~6K params trainable
  sam.mask_decoder.trainable()         # ~4M params trainable

Optional LoRA for image encoder:
  lora_config = {rank: 4, alpha: 4.0, dropout: 0.0}
  inject_lora(sam.image_encoder.blocks, target=['qkv'])
  # Adds ~300K trainable params

Forward pass step-by-step:

  INPUT:
    image_sam: (B, 3, 1024, 1024)    - Normalized image
    coarse_mask: (B, 1024, 1024)     - TransUNet prediction resized

  STEP 1: Image Encoding
    img_embedding = sam.image_encoder(image_sam)
    # Shape: (B, 256, 64, 64)
    # 256 channels, 64x64 spatial (16x downsampled from 1024)

  STEP 2: Prompt Generation
    point_coords, point_labels = extract_points(coarse_mask)
    # point_coords: (B, 2, 2) - [pos_point, neg_point] x [x, y]
    # point_labels: (B, 2) - [1, 0] for [foreground, background]

    box = extract_box(coarse_mask)
    # box: (B, 4) - [x1, y1, x2, y2]

    mask_input = prepare_mask(coarse_mask)
    # mask_input: (B, 1, 256, 256) - logits

  STEP 3: Prompt Encoding
    sparse_embeddings, dense_embeddings = sam.prompt_encoder(
        points=(point_coords, point_labels),
        boxes=box,
        masks=mask_input
    )
    # sparse_embeddings: (B, N_points+2, 256) - point/box embeddings
    # dense_embeddings: (B, 256, 64, 64) - mask embedding

  STEP 4: Mask Decoding
    low_res_masks, iou_predictions = sam.mask_decoder(
        image_embeddings=img_embedding,
        image_pe=sam.prompt_encoder.get_dense_pe(),
        sparse_prompt_embeddings=sparse_embeddings,
        dense_prompt_embeddings=dense_embeddings,
        multimask_output=True
    )
    # low_res_masks: (B, 3, 256, 256) - 3 candidate masks at low resolution
    # iou_predictions: (B, 3) - predicted IoU for each mask

  STEP 5: Mask Selection and Upsampling
    # Select best mask based on predicted IoU
    best_idx = iou_predictions.argmax(dim=1)
    best_mask_low = low_res_masks[batch_idx, best_idx]
    # best_mask_low: (B, 256, 256)

    # Upsample to full resolution
    refined_mask = F.interpolate(
        best_mask_low.unsqueeze(1),
        size=(1024, 1024),
        mode='bilinear'
    ).squeeze(1)
    # refined_mask: (B, 1024, 1024) - logits

    # Apply sigmoid for probability
    refined_prob = torch.sigmoid(refined_mask)
    # refined_prob: (B, 1024, 1024) - probability [0, 1]

  OUTPUT:
    masks_all: (B, 3, 1024, 1024)    - All 3 upsampled candidate masks
    iou_predictions: (B, 3)          - IoU scores for each mask
    refined_mask: (B, 1024, 1024)    - Best mask (highest IoU)

2.6 Loss Function
-----------------
  # Ground truth also resized to 1024x1024
  gt_mask_resized = resize(gt_mask, (1024, 1024))

  # Mask loss (BCE + Dice for each of 3 candidate masks)
  L_bce = BCE_with_logits(masks_all, gt_mask_resized)
  L_dice = 1 - (2 * intersection + 1) / (union + 1)
  L_mask = L_bce + L_dice

  # IoU prediction loss
  iou_actual = compute_iou(sigmoid(masks_all), gt_mask_resized)
  L_iou = MSE(iou_predictions, iou_actual)

  # Total loss
  L_total = 1.0 * L_mask + 1.0 * L_iou

2.7 Validation Metrics Comparison
---------------------------------
During validation, we compute metrics for both coarse and refined masks:

  Coarse metrics: Compare TransUNet prediction vs Ground Truth
  Refined metrics: Compare SAM output vs Ground Truth
  Delta: Refined - Coarse (positive = improvement)

Display format (each epoch):
  ┌────────────────────────────────────────────────────────────┐
  │  [VALIDATION] Coarse (TransUNet) vs Refined (SAM)          │
  │  ============================================================│
  │  Metric       Coarse    Refined      Delta                  │
  │  ------------------------------------------------------------│
  │  Dice         0.7500    0.8200      +0.0700 ↑               │
  │  IoU          0.6000    0.6900      +0.0900 ↑               │
  │  Precision    0.7800    0.8100      +0.0300 ↑               │
  │  Recall       0.7200    0.8300      +0.1100 ↑               │
  │  Accuracy     0.9500    0.9650      +0.0150 ↑               │
  │  ============================================================│
  │  Average improvement: +0.0630 (SAM is helping!)             │
  └────────────────────────────────────────────────────────────┘

TensorBoard logging:
  - val_coarse/dice, val_coarse/iou, ...
  - val_refined/dice, val_refined/iou, ...
  - val_delta/dice, val_delta/iou, ...

Output: checkpoints/sam_finetuned_hybrid/best.pth (shared across all datasets and folds)

Phase 3: End-to-End Joint Optimization
--------------------------------------
Objective: Jointly optimize TransUNet and SAM with full gradient flow.

Model initialization:
  transunet = load(checkpoints/transunet/{D}/fold_{k}/best.pth)  # Per-dataset, per-fold
  sam_refiner = load(checkpoints/sam_finetuned_hybrid/best.pth)  # Shared across datasets
  ultra_refiner = UltraRefiner(transunet, sam_refiner)

Training is performed per-dataset with 5-fold cross-validation, matching Phase 1.

Training Configuration (As Used in Practice):
  ┌─────────────────────────────────────────────────────────────────────────┐
  │  Epochs: 50                                                             │
  │  Batch Size: 2                                                          │
  │  Mask Prompt Style: direct (matching Phase 2)                           │
  │  ROI Cropping: Enabled, expand_ratio=0.5 (matching Phase 2)            │
  │  Loss Weights: 0.1 × L_coarse + 0.9 × L_refined                       │
  │  SAM Learning Rate: 1e-6 (very conservative)                           │
  │  Refined Evaluation Size: 1024 (evaluate at SAM resolution)            │
  │                                                                         │
  │  Progressive Unfreezing Strategy:                                       │
  │  ├── Epochs 0-9:  TransUNet FROZEN, SAM trainable                      │
  │  │                (SAM adapts to TransUNet's fixed outputs)             │
  │  └── Epochs 10-49: TransUNet last 2 decoder layers UNFROZEN            │
  │                    (Joint optimization with partial TransUNet update)   │
  │                                                                         │
  │  TransUNet Freezing: Only last 2 decoder blocks + segmentation head    │
  │  are trainable after unfreezing (encoder + early decoder stay frozen)   │
  │  SAM Freezing: Image encoder always frozen; prompt encoder +           │
  │  mask decoder trainable throughout                                      │
  └─────────────────────────────────────────────────────────────────────────┘

Rationale for Progressive Unfreezing:
  - Epochs 0-9: SAM learns to work with TransUNet's current output distribution
    without TransUNet shifting underneath. This stabilizes early training.
  - Epochs 10-49: TransUNet's last 2 decoder layers are gradually updated,
    allowing it to produce coarse masks better suited for SAM refinement,
    while keeping the encoder features (learned in Phase 1) stable.

Forward pass with gradient flow:
  # TransUNet produces soft mask
  p_coarse = transunet(image_224)                    # (B, H, W)

  # Differentiable prompt generation (gradients flow through)
  prompts = generate_prompts(p_coarse)               # points, boxes, masks

  # SAM refinement (with ROI cropping)
  image_1024 = resize(image_224, 1024)
  roi = extract_roi(p_coarse, expand_ratio=0.5)
  image_roi = crop(image_1024, roi)                  # (B, 3, 1024, 1024)
  img_emb = sam.image_encoder(image_roi)
  sparse_emb, dense_emb = sam.prompt_encoder(prompts_roi)
  p_refined = sam.mask_decoder(img_emb, sparse_emb, dense_emb)
  p_refined_full = paste_back(p_refined, roi)        # Back to full image

Combined loss (heavily weighted toward refinement):
  L_coarse = CE(p_coarse, gt) + Dice(p_coarse, gt)
  L_refined = BCE(p_refined, gt) + Dice(p_refined, gt)
  L_total = 0.1 * L_coarse + 0.9 * L_refined

Gradient flow:
  dL/d(transunet_params) = 0.1 * dL_coarse/d(params)
                         + 0.9 * dL_refined/d(p_refined) × d(p_refined)/d(prompts) × d(prompts)/d(p_coarse) × d(p_coarse)/d(params)

The 0.9 weight on L_refined ensures TransUNet learns to produce coarse masks
optimized for SAM refinement, not just for standalone accuracy.

================================================================================
4. DATASETS AND EVALUATION
================================================================================

Datasets:
---------
| Dataset   | Train | Test | Total | Description                    |
|-----------|-------|------|-------|--------------------------------|
| BUSI      | 518   | 129  | 647   | Breast ultrasound (benign+malignant) |
| BUSBRA    | 1500  | 375  | 1875  | Brazilian breast ultrasound    |
| BUS       | 450   | 112  | 562   | Breast ultrasound dataset      |
| BUS_UC    | 765   | 191  | 956   | UC breast ultrasound           |
| BUS_UCLM  | 130   | 33   | 163   | UCLM breast ultrasound         |

Note: Samples with blank masks (no lesion) are automatically excluded during preprocessing.

Evaluation Protocol:
--------------------
- 5-fold cross-validation within training set for model selection
- Held-out test set (20%) for final evaluation
- Metrics: Dice, IoU (Jaccard), Precision, Recall, Accuracy

================================================================================
5. IMPLEMENTATION DETAILS
================================================================================

Data Pipeline:
- Automatic dataset detection and loading
- K-fold split with deterministic seeding for reproducibility
- Data augmentation: random rotation (±20°), horizontal flip
- TransUNet input: 224x224 grayscale, normalized to [0, 1]
- SAM input: 1024x1024 RGB, normalized with ImageNet stats

Training Configuration:
- Phase 1 (TransUNet): SGD, lr=0.01, poly scheduler, 150 epochs, per-dataset 5-fold CV
- Phase 2 (SAM hybrid): AdamW, lr=1e-4, cosine scheduler, 300 epochs, batch_size=4,
  all 5 datasets combined, 50% real predictions + 50% augmented GT,
  direct mask prompt, ROI crop (expand=0.5), mixed precision (AMP)
- Phase 3 (End-to-end): AdamW, SAM lr=1e-6, cosine scheduler, 50 epochs, batch_size=2,
  per-dataset 5-fold CV, loss weights 0.1 coarse + 0.9 refined,
  direct mask prompt, ROI crop (expand=0.5),
  freeze TransUNet for 10 epochs then unfreeze last 2 decoder layers,
  refined evaluation at 1024 resolution

Logging:
- TensorBoard support for loss, metrics, learning rate
- Separate tracking for coarse/refined/delta metrics
- Beautiful console output with epoch summaries

================================================================================
6. CONTRIBUTIONS
================================================================================

1. First end-to-end differentiable framework integrating SAM for medical image segmentation
2. Novel differentiable prompt generation enabling gradient flow through discrete prompts
3. Prediction-based SAM training using actual coarse network outputs
4. LoRA integration for parameter-efficient SAM adaptation to medical domain
5. Gated residual refinement mechanism that constrains SAM as a controlled error corrector,
   enabling effective refinement without domain-specific finetuning
6. Comprehensive mask augmentation system with 12 primary error types simulating realistic
   TransUNet failure modes, enabling effective Phase 2 training with unlimited diversity:
   - On-the-fly augmentation (no pre-generation required)
   - Soft mask conversion via signed distance transform
   - Resolution path matching for Phase 2 ↔ Phase 3 consistency
   - Configurable presets for different training scenarios
7. Comprehensive evaluation on five breast ultrasound datasets with K-fold CV

Our work establishes a new paradigm for integrating foundation models into task-specific pipelines through differentiable interfaces, with broad applicability beyond medical image segmentation to any domain requiring coarse-to-fine prediction refinement.

The gated residual refinement contribution is particularly significant for practical deployment:
it enables leveraging SAM's boundary refinement capabilities without the computational cost of
Phase 2 finetuning, while protecting against the domain gap problem that causes unfinetuned
foundation models to degrade task-specific predictions.

The mask augmentation system contribution enables effective Phase 2 training by:
- Simulating the full spectrum of segmentation failures (12 error types)
- Generating soft probability masks matching TransUNet's output distribution
- Teaching SAM both "when to correct" and "when to preserve"
- Providing unlimited training diversity through on-the-fly augmentation
