UltraRefiner: End-to-End Differentiable Segmentation Refinement for Medical Image Analysis

Abstract

We present UltraRefiner, a novel end-to-end differentiable framework that unifies coarse segmentation and mask refinement into a single jointly optimizable pipeline for medical image segmentation. Unlike existing two-stage approaches that treat segmentation and refinement as independent processes, UltraRefiner enables gradient flow from the refinement network back to the segmentation backbone, allowing both components to co-adapt during training.

================================================================================
1. ARCHITECTURE OVERVIEW
================================================================================

UltraRefiner consists of two main components connected through a differentiable prompt generation module:

1.1 Coarse Segmentation Network: TransUNet
------------------------------------------
We employ TransUNet as our coarse segmentation backbone, a hybrid CNN-Transformer architecture that combines:
- ResNet-50 encoder for hierarchical feature extraction at multiple scales
- Vision Transformer (ViT-B/16) operating on 14x14 patch embeddings for global context modeling
- CNN decoder with skip connections from encoder stages for precise boundary recovery
- Input: Grayscale ultrasound images resized to 224x224
- Output: 2-class probability map (background/foreground) at full resolution

The TransUNet forward pass:
  x_feat = ResNet50_Encoder(x_input)           # Multi-scale features
  x_patch = Patch_Embedding(x_feat)            # 14x14 patches
  x_trans = Transformer_Blocks(x_patch)        # 12 transformer layers
  x_out = CNN_Decoder(x_trans, skip_features)  # Upsample with skips
  p_coarse = Softmax(x_out)[:, 1]              # Foreground probability

1.2 Refinement Network: SAM (Segment Anything Model)
----------------------------------------------------
We adapt SAM/MedSAM for mask refinement with the following components:
- Image Encoder (ViT-B): Processes 1024x1024 RGB images into 64x64x256 embeddings (FROZEN or LoRA-adapted)
- Prompt Encoder: Encodes points, boxes, and masks into sparse/dense embeddings (TRAINABLE)
- Mask Decoder: Lightweight transformer decoder producing 3 candidate masks with IoU scores (TRAINABLE)

SAM input requirements:
- Image: 1024x1024x3 RGB, normalized with ImageNet mean/std
- Point prompts: (x, y) coordinates with labels (1=foreground, 0=background)
- Box prompts: (x1, y1, x2, y2) bounding box coordinates
- Mask prompts: 256x256 low-resolution mask as logits

================================================================================
2. DIFFERENTIABLE PROMPT GENERATION (Key Innovation)
================================================================================

The core technical contribution is our differentiable prompt generator that converts TransUNet's soft probability outputs into SAM-compatible prompts while maintaining gradient flow.

2.1 Soft-Argmax Point Extraction
--------------------------------
Instead of hard argmax (non-differentiable), we compute the expected centroid:

  Given soft mask M of shape (H, W) with values in [0, 1]:

  mask_sum = sum(M) + epsilon                    # Normalization factor
  x_center = sum(M * X_grid) / mask_sum          # Weighted x-coordinate
  y_center = sum(M * Y_grid) / mask_sum          # Weighted y-coordinate

  point_coords = (x_center, y_center)            # Shape: (B, 1, 2)
  point_labels = 1                               # Positive point

For negative points (background), we compute centroid of inverse mask within bounding box:
  inv_mask = (1 - M) * box_mask
  neg_point = soft_argmax(inv_mask)
  neg_label = 0

2.2 Differentiable Bounding Box Extraction
------------------------------------------
We derive box coordinates from mask projections:

  binary_mask = (M > 0.5)                        # Threshold for boundary detection
  y_proj = max(binary_mask, dim=1)              # Project to y-axis
  x_proj = max(binary_mask, dim=0)              # Project to x-axis

  y_indices = nonzero(y_proj)
  x_indices = nonzero(x_proj)

  box = (x_min, y_min, x_max, y_max)            # Shape: (B, 4)

2.3 Continuous Mask Prompt Encoding
-----------------------------------
We preserve the full probability distribution:

  mask_logits = (M * 2 - 1) * 10                 # Scale [0,1] to [-10, 10]
  mask_input = Resize(mask_logits, (256, 256))  # SAM's expected size

  # Shape: (B, 1, 256, 256)

================================================================================
3. THREE-PHASE TRAINING PIPELINE
================================================================================

Phase 1: TransUNet Pre-training (Per-Dataset, 5-Fold CV)
--------------------------------------------------------
Objective: Train robust coarse segmentation models for each dataset.

For each dataset D in {BUSI, BUSBRA, BUS, BUS_UC, BUS_UCLM}:
  For each fold k in {0, 1, 2, 3, 4}:
    train_idx, val_idx = KFold_Split(D.train, k, n_splits=5)

    model = TransUNet(config='R50-ViT-B_16', num_classes=2)
    model.load_pretrained('R50+ViT-B_16.npz')  # ImageNet pretrained

    optimizer = SGD(lr=0.01, momentum=0.9, weight_decay=1e-4)
    scheduler = PolyLR(power=0.9)

    loss = 0.5 * CrossEntropy + 0.5 * DiceLoss

    Train for 150 epochs
    Save best checkpoint based on validation Dice

Output: checkpoints/transunet/{dataset}/fold_{k}/best.pth

Prediction Generation:
----------------------
After training, we inference each fold's validation set:

For each dataset D:
  For each fold k:
    model = load(checkpoints/transunet/{D}/fold_{k}/best.pth)
    for sample in D.train[val_idx_k]:
      pred = model.inference(sample.image)
      save(pred, predictions/{D}/predictions/{sample.name}.npy)

Result: Every training sample has exactly one prediction (from when it was in validation set)

Phase 2: SAM Fine-tuning (With Actual TransUNet Predictions)
------------------------------------------------------------
Objective: Adapt SAM to refine TransUNet's actual outputs.

Data pairing by filename matching:
  image = dataset/processed/{D}/train/images/{name}.png
  mask = dataset/processed/{D}/train/masks/{name}.png
  coarse_pred = predictions/{D}/predictions/{name}.npy

Model configuration:
  sam = load_sam('vit_b', checkpoint='medsam_vit_b.pth')
  sam.image_encoder.freeze()           # Keep pretrained features
  sam.prompt_encoder.trainable()       # Adapt to coarse mask prompts
  sam.mask_decoder.trainable()         # Adapt refinement behavior

Optional LoRA adaptation for image encoder:
  lora_config = {rank: 4, alpha: 4.0, dropout: 0.0}
  inject_lora(sam.image_encoder.blocks, target=['qkv'])
  # Adds ~300K trainable params to 89M frozen params

SAM Refiner forward pass:
  img_emb = sam.image_encoder(image_1024x1024)

  # Generate prompts from coarse mask
  point_prompt = soft_argmax(coarse_mask)
  box_prompt = extract_box(coarse_mask)
  mask_prompt = resize(coarse_mask, 256x256)

  sparse_emb, dense_emb = sam.prompt_encoder(points, boxes, masks)
  masks_3, iou_scores = sam.mask_decoder(img_emb, sparse_emb, dense_emb)

  refined_mask = masks_3[argmax(iou_scores)]

Loss function:
  L_mask = BCE(refined_mask, gt_mask) + Dice(refined_mask, gt_mask)
  L_iou = MSE(iou_pred, iou_actual)
  L_total = L_mask + L_iou

Validation comparison (displayed each epoch):
  ┌────────────────────────────────────────────────────────────┐
  │  Metric       Coarse    Refined      Delta                 │
  │  Dice         0.7500    0.8200      +0.0700 ↑              │
  │  IoU          0.6000    0.6900      +0.0900 ↑              │
  │  Precision    0.7800    0.8100      +0.0300 ↑              │
  │  Recall       0.7200    0.8300      +0.1100 ↑              │
  └────────────────────────────────────────────────────────────┘

Output: checkpoints/sam_finetuned/fold_{k}/best.pth

Phase 3: End-to-End Joint Optimization
--------------------------------------
Objective: Jointly optimize TransUNet and SAM with full gradient flow.

Model initialization:
  transunet = load(checkpoints/transunet/{D}/fold_{k}/best.pth)
  sam_refiner = load(checkpoints/sam_finetuned/fold_{k}/best.pth)
  ultra_refiner = UltraRefiner(transunet, sam_refiner)

Forward pass with gradient flow:
  # TransUNet produces soft mask
  p_coarse = transunet(image_224)                    # (B, H, W)

  # Differentiable prompt generation (gradients flow through)
  prompts = generate_prompts(p_coarse)               # points, boxes, masks

  # SAM refinement
  image_1024 = resize(image_224, 1024)
  img_emb = sam.image_encoder(image_1024)
  sparse_emb, dense_emb = sam.prompt_encoder(prompts)
  p_refined = sam.mask_decoder(img_emb, sparse_emb, dense_emb)

Combined loss:
  L_coarse = CE(p_coarse, gt) + Dice(p_coarse, gt)
  L_refined = BCE(p_refined, gt) + Dice(p_refined, gt)
  L_total = 0.3 * L_coarse + 0.7 * L_refined

Gradient flow:
  dL/d(transunet_params) = dL_coarse/d(params) + dL_refined/d(p_refined) * d(p_refined)/d(prompts) * d(prompts)/d(p_coarse) * d(p_coarse)/d(params)

This enables TransUNet to learn features that produce coarse masks optimized for SAM refinement.

================================================================================
4. DATASETS AND EVALUATION
================================================================================

Datasets:
---------
| Dataset   | Train | Test | Total | Description                    |
|-----------|-------|------|-------|--------------------------------|
| BUSI      | 518   | 129  | 647   | Breast ultrasound (benign+malignant) |
| BUSBRA    | 1500  | 375  | 1875  | Brazilian breast ultrasound    |
| BUS       | 450   | 112  | 562   | Breast ultrasound dataset      |
| BUS_UC    | 765   | 191  | 956   | UC breast ultrasound           |
| BUS_UCLM  | 130   | 33   | 163   | UCLM breast ultrasound         |

Note: Samples with blank masks (no lesion) are automatically excluded during preprocessing.

Evaluation Protocol:
--------------------
- 5-fold cross-validation within training set for model selection
- Held-out test set (20%) for final evaluation
- Metrics: Dice, IoU (Jaccard), Precision, Recall, Accuracy

================================================================================
5. IMPLEMENTATION DETAILS
================================================================================

Data Pipeline:
- Automatic dataset detection and loading
- K-fold split with deterministic seeding for reproducibility
- Data augmentation: random rotation (±20°), horizontal flip
- TransUNet input: 224x224 grayscale, normalized to [0, 1]
- SAM input: 1024x1024 RGB, normalized with ImageNet stats

Training Configuration:
- TransUNet: SGD, lr=0.01, poly scheduler, 150 epochs
- SAM fine-tune: AdamW, lr=1e-4, cosine scheduler, 100 epochs
- End-to-end: AdamW, lr=1e-5, cosine scheduler, 100 epochs

Logging:
- TensorBoard support for loss, metrics, learning rate
- Separate tracking for coarse/refined/delta metrics
- Beautiful console output with epoch summaries

================================================================================
6. CONTRIBUTIONS
================================================================================

1. First end-to-end differentiable framework integrating SAM for medical image segmentation
2. Novel differentiable prompt generation enabling gradient flow through discrete prompts
3. Prediction-based SAM training using actual coarse network outputs
4. LoRA integration for parameter-efficient SAM adaptation to medical domain
5. Comprehensive evaluation on five breast ultrasound datasets with K-fold CV

Our work establishes a new paradigm for integrating foundation models into task-specific pipelines through differentiable interfaces, with broad applicability beyond medical image segmentation to any domain requiring coarse-to-fine prediction refinement.
